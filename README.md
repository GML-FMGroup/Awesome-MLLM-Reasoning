# Awesome-MLLM-Reasoning

We have witnessed the tremendous potential of pure reinforcement learning (RL) in enhancing LLM reasoning capabilities, and a growing body of research is now extending this potential to the multimodal domain. This repository will continuously update the latest papers, covering how reinforcement learning techniques can optimize reasoning performance in multimodal tasks (e.g., visual question answering, cross-modal reasoning, and more).

**üöÄ Here, you‚Äôll be at the forefront of "RL + MLLM Reasoning" research!**
Whether you're a researcher, engineer, or student, this repository will help you quickly master **how RL empowers complex reasoning and decision-making in MLLMs**.

Watch & Star üåü‚Äîlet‚Äôs explore the future of multimodal intelligence together!

## Papers üìÑ
1. **[VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning.](https://arxiv.org/abs/2507.13348)** [[code](https://github.com/dvlab-research/VisionThink)]
    *Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia.* Preprint'25

1. **[M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning.](https://arxiv.org/abs/2507.08306)** [[code](https://github.com/inclusionAI/M2-Reasoning)]
    *Inclusion AI: Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, Yi Yuan, Yifan Mao, Yuting Xiao, Ziping Ma.* Preprint'25

1. **[The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs.](https://arxiv.org/abs/2507.07562)** [[code](https://github.com/JierunChen/SFT-RL-SynergyDilemma)]
    *Jierun Chen, Tiezheng Yu, Haoli Bai, Lewei Yao, Jiannan Wu, Kaican Li, Fei Mi, Chaofan Tao, Lei Zhu, Manyi Zhang, Xiaohui Li, Lu Hou, Lifeng Shang, Qun Liu.* Preprint'25

1. **[VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning.](https://www.arxiv.org/abs/2507.22607)** [[code](https://github.com/alibaba-damo-academy/VL-Cogito)]
    *Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, Hao Zhang, Yu Rong.* Preprint'25

1. **[Perception-Aware Policy Optimization for Multimodal Reasoning
.](https://arxiv.org/abs/2507.06448)** [[project](https://mikewangwzhl.github.io/PAPO/)]
    *Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji.* Preprint'25

1. **[Can Multimodal Foundation Models Understand Schematic Diagrams?
An Empirical Study on Information-Seeking QA over Scientific Papers
.](https://arxiv.org/abs/2507.10624)** [[code](https://github.com/yilunzhao/MISS-QA)]
    *Zheng Zhang.* Preprint'25

1. **[Comprehension Without Competence: Architectural Limits
of LLMs in Symbolic Computation and Reasoning
.](https://arxiv.org/abs/2507.07998)**
    *Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei.* Preprint'25


1. **[True Multimodal In-Context Learning Needs
Attention to the Visual Context
.](https://arxiv.org/abs/2507.15807)** [[project](https://chenxshuo.github.io/true-micl-colm/)]
    *Shuo Chen, Jianzhe Liu, Zhen Han, Yan Xia, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu.* Preprint'25

1. **[PyVision: Agentic Vision with Dynamic Tooling
.](https://arxiv.org/abs/2507.07998)** [[project](https://agent-x.space/pyvision/)]
    *Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei.* Preprint'25
    
1. **[ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning.](https://arxiv.org/abs/2506.03596)** [[code](https://github.com/Maplebb/ControlThinker)]
    *Feng Han, Yang Jiao, Shaoxiang Chen, Junhao Xu, Jingjing Chen, Yu-Gang Jiang.* Preprint'25

1. **[Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning.](https://arxiv.org/abs/2506.04034)**  [[project](https://rexthinker.github.io/)]
    *Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Lei Zhang.* Preprint'25

1. **[Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning.](https://arxiv.org/abs/2506.04207)** [[code](https://github.com/CSfufu/Revisual-R1)]
    *Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng.* Preprint'25

1. **[ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs.](https://arxiv.org/abs/2506.10128)** [[code](https://github.com/si0wang/ViCrit)]
    *Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, Linjie Li, Furong Huang, Lijuan Wang.* Preprint'25

1. **[Play to Generalize: Learning to Reason Through Game Play.](https://arxiv.org/abs/2506.08011)** [[project](https://yunfeixie233.github.io/ViGaL/)]
    *Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei.* Preprint'25

1. **[SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis.](https://www.arxiv.org/abs/2506.02096)** [[code](https://github.com/NUS-TRAIL/SynthRL)] [[model](https://huggingface.co/collections/Jakumetsu/synthrl-6839d265136fa9ca717105c5)]
    *Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh.* Preprint'25

1. **[Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning.](https://arxiv.org/abs/2506.04559)** [[code](https://github.com/gyhdog99/RACRO2/)]
    *Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Xin Jin, Zhenguo Li, James T. Kwok, Yu Zhang.* Preprint'25

1. **[Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation.](https://arxiv.org/abs/2505.18842)** 
    Jiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu. Preprint'25

1. **[More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models.](https://arxiv.org/abs/2505.21523)**  [[project](https://mlrm-halu.github.io/)]
    *Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu.* Preprint'25

1. **[STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs.](https://arxiv.org/abs/2505.15804)**  [[code](https://github.com/zongzhao23/STAR-R1)]
    *Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang*. Preprint'25

1. **[Visual Agentic Reinforcement Fine-Tuning.](https://arxiv.org/abs/2505.14246)** [[code](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT)]
    *Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang.* Preprint'25

1. **[Visual Planning: Let's Think Only with Images.](https://arxiv.org/abs/2505.11409)**  [[code](https://github.com/yix8/VisualPlanning)]
    *Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vuliƒá.* Preprint'25

1. **[GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning.](https://arxiv.org/pdf/2505.11049)**
    *Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, Bryan Hooi.* Preprint'25

1. **[OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning.](https://arxiv.org/abs/2505.08617)**  [[code](https://github.com/zhaochen0110/OpenThinkIMG)]
    *Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, Yu Cheng.* Preprint'25

1. **[DanceGRPO: Unleashing GRPO on Visual Generation.](https://arxiv.org/abs/2505.07818)** [[project](https://dancegrpo.github.io/)] [[code](https://github.com/XueZeyue/DanceGRPO)]
    *Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo.* Preprint'25

1. **[Flow-GRPO: Training Flow Matching Models via Online RL.](https://www.arxiv.org/abs/2505.05470)** [[code](https://github.com/yifan123/flow_grpo)]
    *Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, Wanli Ouyang.* Preprint'25

1. **[X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains.](https://arxiv.org/abs/2505.03981)** [[code](https://github.com/microsoft/x-reasoner)]
    *Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, Hoifung Poon.* Preprint'25

1. **[T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT.](https://arxiv.org/abs/2505.00703)** [[code](https://github.com/CaraJ7/T2I-R1)]
    *Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li.* Preprint'25

1. **[Fast-Slow Thinking for Large Vision-Language Model Reasoning.](https://arxiv.org/abs/2504.18458)** 
    *Wenyi Xiao, Leilei Gan, Weilong Dai, Wanggui He, Ziwei Huang, Haoyuan Li, Fangxun Shu, Zhelun Yu, Peng Zhang, Hao Jiang, Fei Wu.* Preprint'25

1. **[SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models.](https://arxiv.org/abs/2504.11468)** [[project](https://ucsc-vlaa.github.io/VLAA-Thinking/)] [[code](https://github.com/UCSC-VLAA/VLAA-Thinking)] [[dataset](https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking)]
    *Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, Cihang Xie.*  Preprint'25

1. **[Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning.](https://arxiv.org/abs/2504.16656)**  [[code](https://huggingface.co/Skywork/Skywork-R1V2-38B)] 
    *Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou*. Preprint'25

1. **[Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding.](https://arxiv.org/abs/2502.11492)** [[code](https://github.com/SalesforceAIResearch/CogAlign)] 
    *Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu.* Preprint'25

1. **[Compile Scene Graphs with Reinforcement Learning.](https://www.arxiv.org/pdf/2504.13617)** [[code](https://github.com/gpt4vision/R1-SGG)] 
    *Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen.* Preprint'25

1. **[NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation.](https://arxiv.org/abs/2504.13055)** [[code](https://github.com/John-AI-Lab/NoisyRollout)] 
    *Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, Michael Qizhe Shieh.* Preprint'25

1. **[Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning.](https://arxiv.org/abs/2504.12680)** [[project](https://embodiedcity.github.io/Embodied-R/)] [[code](https://github.com/EmbodiedCity/Embodied-R.code)] 
    *Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, Wenwu Zhu.* Preprint'25

1. **[SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement.](https://arxiv.org/abs/2504.07934)** [[code](https://github.com/si0wang/ThinkLite-VL)] 
    *Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang. Preprint'25*

1. **[Perception-R1: Pioneering Perception Policy with Reinforcement Learning.](https://arxiv.org/abs/2504.07954)** [[code](https://github.com/linkangheng/PR1)] 
    *En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, Wenbing Tao*. Preprint'25

1. **[VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning.](https://arxiv.org/abs/2504.08837)** [[project](https://tiger-ai-lab.github.io/VL-Rethinker/)] [[code](https://github.com/TIGER-AI-Lab/VL-Rethinker/)]  [[model](https://huggingface.co/TIGER-Lab/VL-Rethinker-7B)]
    *Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen*. Preprint'25

1. **[Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning.](https://arxiv.org/abs/2503.16188v2)** [[code](https://github.com/minglllli/CLS-RL)] 
    *Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Kaipeng Zhang.* Preprint'25

1. **[VisRL: Intention-Driven Visual Perception via Reinforced Reasoning.](https://arxiv.org/abs/2503.07523)** [[code](https://github.com/zhangquanchen/VisRL)] 
    *Zhangquan Chen, Xufang Luo, Dongsheng Li.* Preprint'25

1. **[Improved Visual-Spatial Reasoning via R1-Zero-Like Training.](https://arxiv.org/pdf/2504.00883)** [[code](https://github.com/zhijie-group/R1-Zero-VSI)] 
    *Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng*. Preprint'25

1. **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning.](https://arxiv.org/abs/2503.22679)**  [[code](https://github.com/lwq20020127/Q-Insight)] 
    *Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang*. Preprint'25
    
1. **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning.](https://arxiv.org/pdf/2503.21620)** [[code](https://github.com/lll6gg/UI-R1)] 
    *Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, Hongsheng Li*. Preprint'25
    
1. **[Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning.](https://arxiv.org/abs/2504.00907)**
    *Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi.* Preprint'25
    
1. **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1.](https://arxiv.org/pdf/2503.24376)**
    *Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu.* Preprint'25
    
1. **[Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models.](https://arxiv.org/pdf/2503.16724)**
    *Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay.* Preprint'25
    
1. **[GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing.](https://arxiv.org/abs/2503.10639)** 
    *Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li.* Preprint'25
    
1. **[Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme.](https://www.arxiv.org/abs/2504.02587)** [[code](https://github.com/GAIR-NLP/MAYE)]
    *Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu.* Preprint'25
    
1. **[LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs.](https://arxiv.org/abs/2501.06186)** [[project](https://mbzuai-oryx.github.io/LlamaV-o1/)] [[code](https://github.com/mbzuai-oryx/LlamaV-o1)] [[model](https://huggingface.co/omkarthawakar/LlamaV-o1)]
    *Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan.* Preprint'25
    
1. [**LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL.**](https://arxiv.org/pdf/2503.07536) [[code](https://github.com/TideDra/lmm-r1)]
     *Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang.* Preprint'25

1. [**R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model.**](https://arxiv.org/abs/2503.05132)[[code](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)] [[model](https://huggingface.co/turningpoint-ai/VisualThinker-R1-Zero)]
     *Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang.* Preprint'25

1. [**R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning.**](https://arxiv.org/abs/2503.05379) [[code](https://github.com/HumanMLLM/R1-Omni)] [[model](https://huggingface.co/StarJiaxing/R1-Omni-0.5B)]
     *Jiaxing Zhao, Xihan Wei, Liefeng Bo*. Preprint'25

1. [**Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement.**](https://arxiv.org/abs/2503.06520) [[code](https://github.com/dvlab-research/Seg-Zero)] [[model](https://huggingface.co/Ricky06662/Seg-Zero-7B)]
     *Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia*. Preprint'25

1. [**Visual-RFT: Visual Reinforcement Fine-Tuning.**](https://arxiv.org/abs/2503.01785) [[code](https://github.com/Liuziyu77/Visual-RFT)]
     *Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang*. Preprint'25

1. [**MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning.**](https://arxiv.org/abs/2502.19634)
     *Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert*. Preprint'25

1. [**Virgo: A Preliminary Exploration on Reproducing o1-like MLLM.**](https://arxiv.org/abs/2501.01904v2)[[code](https://github.com/RUCAIBox/Virgo)]
     *Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen.* Preprint'25

1. [**Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step.**](https://arxiv.org/abs/2501.13926)[[code](https://github.com/ZiyuGuo99/Image-Generation-CoT)] [[model](https://huggingface.co/ZiyuG/Image-Generation-CoT)]
     Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng. Preprint'25

1. [**R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization.**](https://arxiv.org/abs/2503.10615)[[code](https://github.com/Fancy-MLLM/R1-Onevision)] [[model](https://huggingface.co/spaces/Fancy-MLLM/R1-Onevision)]
     *Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen.* Preprint'25

1. [**Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models.**](https://arxiv.org/abs/2503.06749) [[code](https://github.com/Osilly/Vision-R1)]
      *Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin*. Preprint'25

1. **[Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models.](https://arxiv.org/abs/2411.14432)** [[code](https://github.com/dongyh20/Insight-V)] [[model](https://huggingface.co/collections/THUdyh/insight-v-673f5e1dd8ab5f2d8d332035)]

    *Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu.* Preprint'24

1. **[LLaVA-CoT: Let Vision Language Models Reason Step-by-Step.](https://arxiv.org/abs/2411.10440)** [[code](https://github.com/PKU-YuanGroup/LLaVA-CoT)] [[model](https://huggingface.co/Xkev/Llama-3.2V-11B-cot)]

    *Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan.* Preprint'24

1. **[Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.](https://arxiv.org/abs/2406.09403)** [[project](https://visualsketchpad.github.io/)] [[code](https://github.com/Yushi-Hu/VisualSketchpad)]
    *Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna.* Preprint'24
    
1. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319)[[code](https://github.com/HJYao00/Mulberry)] 
    *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*. Preprint'24
    
1. [**Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization.**](https://arxiv.org/abs/2411.10442)[[project](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/)] [[code](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)] [[model](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO)]
    *Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, Jifeng Dai*. Preprint'24
    
1. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319)[[code](https://github.com/HJYao00/Mulberry)] 
    *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*. Preprint'24



## Benchmark & Evaluationü§ó


1. **[Zebra-CoT: A Dataset for Interleaved
Vision-Language Reasoning
.](https://arxiv.org/abs/2507.16746)** [[code](https://github.com/multimodal-reasoning-lab/Bagel-Zebra-CoT)]
    *Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum.* Preprint'25
    
1. **[Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence.](https://arxiv.org/abs/2505.23747)**  [[project](https://diankun-wu.github.io/Spatial-MLLM/)]
   *Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan.* Preprint'25
   
1. **[MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs.](https://arxiv.org/abs/2505.21327)**  [[project](https://alpha-innovator.github.io/mmereasoning.github.io/)]
   *Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, Xiangyu Yue.* Preprint'25
   
1. **[MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks.](https://arxiv.org/abs/2505.16459)**  [[project](https://mmmr-benchmark.github.io/)]
   *Guiyao Tie, Xueyang Zhou, Tianhe Gu, Ruihang Zhang, Chaoran Hu, Sizhe Zhang, Mengqu Sun, Yan Zhang, Pan Zhou, Lichao Sun.* Preprint'25
   
1. **[VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection.](https://www.arxiv.org/abs/2505.20289)**  [[project](https://oodbag.github.io/vista_web/)]
   *Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, Yong Jae Lee.* Preprint'25
   
1. **[RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs.](https://arxiv.org/abs/2505.16770)**  [[project](https://evalmodels.github.io/rbenchv/)]
   *Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, Shi-Min Hu.* Preprint'25
   
1. **[LMGAME-BENCH: How Good are LLMs at Playing Games?](https://arxiv.org/pdf/2505.15146)** 
   *Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang.* Preprint'25
   
1. **[GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling.](https://www.arxiv.org/abs/2505.00063)** 
   *Siqi Li, Yufan Shen, Xiangnan Chen, Jiayi Chen, Hengwei Ju, Haodong Duan, Song Mao, Hongbin Zhou, Bo Zhang, Pinlong Cai, Licheng Wen, Botian Shi, Yong Liu, Xinyu Cai, Yu Qiao.* Preprint'25
   
1. **[MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science.](https://arxiv.org/abs/2501.10768)** 
   *Erle Zhu, Yadi Liu, Zhe Zhang, Xujun Li, Jin Zhou, Xinjie Yu, Minlie Huang, Hongning Wang.* Preprint'25
   
1. **[VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models.](https://arxiv.org/abs/2502.16435)**  [[code](https://github.com/CUHK-ARISE/VisFactor)]
   *Jen-Tse Huang, Dasen Dai, Jen-Yuan Huang, Youliang Yuan, Xiaoyuan Liu, Wenxuan Wang, Wenxiang Jiao, Pinjia He, Zhaopeng Tu.* Preprint'25
   
1. **[Are Large Vision Language Models Good Game Players?](https://www.arxiv.org/abs/2503.02358)**  [[code](https://github.com/xinke-wang/LVLM-Playground)]
   *Xinyu Wang, Bohan Zhuang, Qi Wu.* Preprint'25
   
1. **[VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models.](https://arxiv.org/abs/2504.15279)**  [[project](https://visulogic-benchmark.github.io/VisuLogic/)] [[code](https://github.com/VisuLogic-Benchmark/VisuLogic-Train)] [[dataset](https://huggingface.co/datasets/VisuLogic/VisuLogic)]
   *Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, Jinguo Zhu.* Preprint'25
   
1. **[EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges.](https://arxiv.org/pdf/2502.08859)**  [[project](https://scale.com/leaderboard/enigma_eval)]
   *Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks.* Preprint'25
   
1. **[NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models.](https://arxiv.org/abs/2407.10380)** 
   *Pranshu Pandya, Vatsal Gupta, Agney S Talwarr, Tushar Kataria, Dan Roth, Vivek Gupta.* Preprint'25
   
1. **[Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models.](https://arxiv.org/abs/2503.19707)**  [[code](https://github.com/stogiannidis/srbench)] [[dataset](https://huggingface.co/datasets/stogiannidis/srbench)]
   *Ilias Stogiannidis, Steven McDonagh, Sotirios A. Tsaftaris.*  Preprint'25
   
1. **[Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark.](https://arxiv.org/abs/2501.05444)** [[project](https://emma-benchmark.github.io/)] [[code](https://github.com/hychaochao/EMMA)] [[dataset](https://huggingface.co/datasets/luckychao/EMMA)]
   *Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, Yu Cheng.* Preprint'25
   
1. **[VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge.](https://arxiv.org/abs/2504.10342)** [[project](https://neulab.github.io/VisualPuzzles/)] [[code](https://github.com/neulab/VisualPuzzles)] [[dataset](https://huggingface.co/datasets/neulab/VisualPuzzles)]
   *Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue.*  Preprint'25
   
1. **[V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models.](https://arxiv.org/pdf/2504.06148)**  [[code](https://github.com/CSU-JPG/V-MAGE)] 
   *Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, Lijuan Wang.*  Preprint'25
   
1. **[CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation.](https://arxiv.org/pdf/2504.00043)** [[code](https://github.com/SeanLeng1/CrossWordBench)] [[dataset](https://huggingface.co/datasets/HINT-lab/CrossWordBench)]
   *Jixuan Leng, Chengsong Huang, Langlin Huang, Bill Yuchen Lin, William W. Cohen, Haohan Wang, Jiaxin Huang.*  Preprint'25
   
1. **[Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency.](https://www.arxiv.org/abs/2504.18589)**
   *Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao.*  Preprint'25
   
1. **[VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models.](https://arxiv.org/abs/2504.15279)** [[project](https://visulogic-benchmark.github.io/VisuLogic/)] [[code](https://github.com/VisuLogic-Benchmark/VisuLogic-Train)] [[dataset](https://huggingface.co/datasets/VisuLogic/VisuLogic)]
   *Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, Jinguo Zhu.* Preprint'25
   
1. **[Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation.](https://www.arxiv.org/abs/2504.17207)** [[project](https://apc-vlm.github.io/)] [[code](https://github.com/KAIST-Visual-AI-Group/APC-VLM)]
   *Phillip Y. Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, Minhyuk Sung*  Preprint'25
   
1. **[Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark.](https://arxiv.org/abs/2504.16427)**  [[code](https://github.com/thuiar/MMLA)] 
   *Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang.* Preprint'25
   
1. **[ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering.](https://arxiv.org/abs/2504.05506)** [[code](https://github.com/vis-nlp/ChartQAPro)] 
   *Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, Megh Thakkar, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty.* Preprint'25
   
1. **[GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning.](https://arxiv.org/abs/2504.12597)** 
   *Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng.* Preprint'25
   
1. **[VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models.](https://arxiv.org/abs/2503.23064)** [[project](https://yufan-ren.com/subpage/VGRP-Bench/)] [[code](https://github.com/ryf1123/VGRP-Bench)] [[dataset](https://huggingface.co/datasets/VGRP-Bench/VGRP-Bench)]
   *Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine S√ºsstrunk, Filippos Kokkinos.* Preprint'25
   
1. **[VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning.](https://arxiv.org/abs/2504.05782)** [[project](https://vlm-reasoning.github.io/VCR-Bench/)]
   *Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, Feng Zhao.* Preprint'25
   
1. **[MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models.](https://arxiv.org/abs/2504.05782)** [[dataset](https://github.com/LanceZPF/MDK12)]
   
    *Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, Jianwen Sun, Haoquan Zhang, Zizhen Li, Xiaofeng Mao, Wangbo Zhao, Kai Wang, Xiaojun Chang, Wenqi Shao, Yang You, Kaipeng Zhang.* Preprint'25
    
1. **[LIVEVQA: Live Visual Knowledge Seeking.](https://arxiv.org/pdf/2504.05288)** [[dataset](https://huggingface.co/papers/2504.05288)]
    *Mingyang Fu, Yuyang Peng, Benlin Liu, Yao Wan, Dongping Chen.* CVPR 2025
    
1. **[MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts.](https://arxiv.org/abs/2502.20808)** [[project](https://eternal8080.github.io/MV-MATH.github.io/)] [[code](https://github.com/eternal8080/MV-MATH)]
    *Peijie Wang, Zhongzhi Li, Dekang Ran, Fei Yin, Chenglin Liu.* CVPR 2025
    
1. [**MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models.**](https://arxiv.org/abs/2502.00698) [[project](https://acechq.github.io/MMIQ-benchmark/)] [[code](https://github.com/AceCHQ/MMIQ/tree/main/)] [[dataset](https://huggingface.co/datasets/huanqia/MM-IQ)]
    *Huanqia Cai, Yijun Yang, Winston Hu*. Preprint'25

1. [**MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency.**](https://arxiv.org/abs/2502.09621) [[project](https://mmecot.github.io/)] [[code](https://github.com/CaraJ7/MME-CoT)] [[dataset](https://huggingface.co/datasets/CaraJ/MME-CoT)]
    *Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li*. Preprint'25

1. [**ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models.**](https://arxiv.org/abs/2502.09696) [[project](https://zerobench.github.io/)] [[code](https://github.com/jonathan-roberts1/zerobench)] [[dataset](https://huggingface.co/datasets/jonathan-roberts1/zerobench)]
    *Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, David I. Atkinson, Aaditya Baranwal, Alexandru Coca, Mikah Dang, Sebastian Dziadzio, Jakob D. Kunz, Kaiqu Liang, Alexander Lo, Brian Pulfer, Steven Walton, Charig Yang, Kai Han, Samuel Albanie*. Preprint'25

1. [**MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning.**](https://arxiv.org/abs/2503.07365) [[code](https://github.com/ModalMinds/MM-EUREKA)] [[model](https://huggingface.co/FanqingM/MM-Eureka-8B)] [[dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)]
    *Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao.* Preprint'25

1. [**MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts.**](https://arxiv.org/abs/2310.02255) [[project](https://mathvista.github.io/)] 
    *Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao.* ICLR 2024

1. [**MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark.**](https://arxiv.org/abs/2408.07543) [[code](https://github.com/PKU-Baichuan-MLSystemLab/MathScape)] 
    *Minxuan Zhou, Hao Liang, Tianpeng Li, Zhiyu Wu, Mingan Lin, Linzhuang Sun, Yaqi Zhou, Yan Zhang, Xiaoqin Huang, Yicong Chen, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou* Preprint'24

1. [**Template-Driven LLM-Paraphrased Framework for Tabular Math Word Problem Generation.**](https://arxiv.org/abs/2412.15594v1) [[project](https://acechq.github.io/MMIQ-benchmark/)] [[code](https://github.com/AceCHQ/MMIQ/tree/main/)] [[dataset](https://huggingface.co/datasets/huanqia/MM-IQ)]
    *Xiaoqiang Kang, Zimu Wang, Xiaobo Jin, Wei Wang, Kaizhu Huang, Qiufeng Wang.* Preprint'24

1. **[How Far Are We from Intelligent Visual Deductive Reasoning?](https://arxiv.org/abs/2403.04732)** 
    *Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly.* Preprint'24

1. [**Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset.**](https://arxiv.org/abs/2402.14804) [[project](https://mathllm.github.io/mathvision/)] [[code](https://github.com/mathllm/MATH-V)] [[dataset](https://huggingface.co/datasets/MathLLMs/MathVision)]
    *Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li* Preprint'24

1. **[VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning.](https://arxiv.org/abs/2410.22995)** 
    *Jingkun Ma, Runzhe Zhan, Derek F. Wong, Yang Li, Di Sun, Hou Pong Chan, Lidia S. Chao.* Preprint'24

1. [**MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?**](Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li) [[project](https://mathverse-cuhk.github.io/)] 
    *Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li* . *ECCV 2024*

1. [**BLINK: Multimodal Large Language Models Can See but Not Perceive.**](https://arxiv.org/abs/2404.12390) [[project](https://zeyofu.github.io/blink/)] [[code](https://github.com/zeyofu/BLINK_Benchmark)] [[dataset](https://huggingface.co/datasets/BLINK-Benchmark/BLINK)]
    *Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna*. ECCV 2024

1. [**Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models.**](https://arxiv.org/abs/2406.17294) [[code](https://github.com/HZQ950419/Math-LLaVA)]
    *Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee.* Preprint'24

1. **[HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks.](https://arxiv.org/abs/2410.12381)** [[code](https://github.com/HumanEval-V/HumanEval-V-Benchmark)] 
    *Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, Jacky Keung.* Preprint'24

1. **[FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts.](https://arxiv.org/abs/2406.19237)** 
    *Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth.* Preprint'24

1. **[GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training.](https://arxiv.org/abs/2412.11863)** [[code](https://github.com/Alpha-Innovator/GeoX)] 
   
    *Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, Bo Zhang.* Preprint'24
    
1. [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension.**](https://arxiv.org/abs/2307.16125) [[code](https://github.com/AILab-CVC/SEED-Bench)]
    *Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan*. Preprint'23

1. [**ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning.**](https://arxiv.org/abs/2203.10244) [[code](https://github.com/vis-nlp/ChartQA)]
    *Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque.* Preprint'22

1. [**Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.**](http://arxiv.org/abs/2209.09513) [[code](https://github.com/lupantech/ScienceQA)]
     *Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan.* NeurIPS 2022

1. [**GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning.**](https://arxiv.org/abs/2105.14517)  [[code](https://github.com/chen-judge/GeoQA)]
     *Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, Liang Lin.* Preprint'21

1. [**FigureQA: An Annotated Figure Dataset for Visual Reasoning.**](https://arxiv.org/abs/1710.07300)
    *Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, Yoshua Bengio.*  Preprint'17
    
    

## Surveyüìñ

1. [**Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models.**](https://arxiv.org/abs/2505.18536)  [[code](https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs)] 
   *Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, Xueqian Wang*. Preprint'25
1. [**Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models.**](https://arxiv.org/abs/2505.04921)  [[code](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models)] 
   *Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, Min Zhang.* Preprint'25
1. [**Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey.**](https://arxiv.org/abs/2503.12605)  [[code](http://github.com/yaotingwangofficial/Awesome-MCoT)] 
   *Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, William Wang, Ziwei Liu, Jiebo Luo, Hao Fei*. Preprint'25
2. [**Mind with Eyes: from Language Reasoning to Multimodal Reasoning.**](https://arxiv.org/abs/2503.18071) 
   *Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, Jitao Sang.* Preprint'25
1. [**A Survey on Multimodal Benchmarks: In the Era of Large AI Models.**](https://arxiv.org/pdf/2409.18142) 
   *Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, Long Chen*. Preprint'24

## Contributing‚≠ê

This is an active repository and your contributions are always welcome!
