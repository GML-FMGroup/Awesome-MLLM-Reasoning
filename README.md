# Awesome-MLLMs-Reasoning

## Papers üìÑ

1. **[Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models.](https://arxiv.org/abs/2411.14432)** [[code](https://github.com/dongyh20/Insight-V)] [[model](https://huggingface.co/collections/THUdyh/insight-v-673f5e1dd8ab5f2d8d332035)]

    *Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu.* Preprint'24

1. **[LLaVA-CoT: Let Vision Language Models Reason Step-by-Step.](https://arxiv.org/abs/2411.10440)** [[code](https://github.com/PKU-YuanGroup/LLaVA-CoT)] [[model](https://huggingface.co/Xkev/Llama-3.2V-11B-cot)]

    *Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan.* Preprint'24

1. **[Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.](https://arxiv.org/abs/2406.09403)** [[project](https://visualsketchpad.github.io/)] [[code](https://github.com/Yushi-Hu/VisualSketchpad)]
    *Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna.* Preprint'24
    
1. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319)[[code](https://github.com/HJYao00/Mulberry)] 
    *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*. Preprint'24
    
1. [**Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization.**](https://arxiv.org/abs/2411.10442)[[project](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/)] [[code](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)] [[model](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO)]
    *Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, Jifeng Dai*. Preprint'24
    
1. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319)[[code](https://github.com/HJYao00/Mulberry)] 
    *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*. Preprint'24
    
1. **[LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs.](https://arxiv.org/abs/2501.06186)** [[project](https://mbzuai-oryx.github.io/LlamaV-o1/)] [[code](https://github.com/mbzuai-oryx/LlamaV-o1)] [[model](https://huggingface.co/omkarthawakar/LlamaV-o1)]
    *Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan.* Preprint'25
    
1. [**LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL.**](https://arxiv.org/pdf/2503.07536) [[code](https://github.com/TideDra/lmm-r1)]
    *Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang.* Preprint'25
    
1. [**R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model.**](https://arxiv.org/abs/2503.05132)[[code](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)] [[model](https://huggingface.co/turningpoint-ai/VisualThinker-R1-Zero)]
    *Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang.* Preprint'25

1. [**R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning.**](https://arxiv.org/abs/2503.05379) [[code](https://github.com/HumanMLLM/R1-Omni)] [[model](https://huggingface.co/StarJiaxing/R1-Omni-0.5B)]
    *Jiaxing Zhao, Xihan Wei, Liefeng Bo*. Preprint'25

1. [**Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement.**](https://arxiv.org/abs/2503.06520) [[code](https://github.com/dvlab-research/Seg-Zero)] [[model](https://huggingface.co/Ricky06662/Seg-Zero-7B)]
     *Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia*. Preprint'25

1. [**Visual-RFT: Visual Reinforcement Fine-Tuning.**](https://arxiv.org/abs/2503.01785) [[code](https://github.com/Liuziyu77/Visual-RFT)]
     *Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang*. Preprint'25

1. [**MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning.**](https://arxiv.org/abs/2502.19634)
     *Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert*. Preprint'25

1. [**Virgo: A Preliminary Exploration on Reproducing o1-like MLLM.**](https://arxiv.org/abs/2501.01904v2)[[code](https://github.com/RUCAIBox/Virgo)]
     *Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen.* Preprint'25

1. [**Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step.**](https://arxiv.org/abs/2501.13926)[[code](https://github.com/ZiyuGuo99/Image-Generation-CoT)] [[model](https://huggingface.co/ZiyuG/Image-Generation-CoT)]
     Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng. Preprint'25

1. [**R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization.**](https://arxiv.org/abs/2503.10615)[[code](https://github.com/Fancy-MLLM/R1-Onevision)] [[model](https://huggingface.co/spaces/Fancy-MLLM/R1-Onevision)]
     *Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen.* Preprint'25

1. [**Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models.**](https://arxiv.org/abs/2503.06749) [[code](https://github.com/Osilly/Vision-R1)]
     *Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin*. Preprint'25



## Benchmark & Evaluationü§ó

1. [**FigureQA: An Annotated Figure Dataset for Visual Reasoning.**](https://arxiv.org/abs/1710.07300)
    *Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, Yoshua Bengio.*  Preprint'17
    
1. [***GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning*.**](https://arxiv.org/abs/2105.14517)  [[code](https://github.com/chen-judge/GeoQA)]
    *Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, Liang Lin.* Preprint'21
    
1. [**Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.**](http://arxiv.org/abs/2209.09513) [[code](https://github.com/lupantech/ScienceQA)]
    *Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan.* NeurIPS 2022
    
1. [**ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning.**](https://arxiv.org/abs/2203.10244) [[code](https://github.com/vis-nlp/ChartQA)]
    *Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque.* Preprint'22

1. [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension.**](https://arxiv.org/abs/2307.16125) [[code](https://github.com/AILab-CVC/SEED-Bench)]
    *Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan*. Preprint'23
    
1. [**MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts.**](https://arxiv.org/abs/2310.02255) [[project](https://mathvista.github.io/)] 
    *Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao.* ICLR 2024
    
1. [**Template-Driven LLM-Paraphrased Framework for Tabular Math Word Problem Generation.**](https://arxiv.org/abs/2412.15594v1) [[project](https://acechq.github.io/MMIQ-benchmark/)] [[code](https://github.com/AceCHQ/MMIQ/tree/main/)] [[dataset](https://huggingface.co/datasets/huanqia/MM-IQ)]
    *Xiaoqiang Kang, Zimu Wang, Xiaobo Jin, Wei Wang, Kaizhu Huang, Qiufeng Wang.* Preprint'24
    
1. [**BLINK: Multimodal Large Language Models Can See but Not Perceive.**](https://arxiv.org/abs/2404.12390) [[project](https://zeyofu.github.io/blink/)] [[code](https://github.com/zeyofu/BLINK_Benchmark)] [[dataset](https://huggingface.co/datasets/BLINK-Benchmark/BLINK)]
    *Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna*. ECCV 2024

1. **[MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts.](https://arxiv.org/abs/2502.20808)** [[project](https://eternal8080.github.io/MV-MATH.github.io/)] [[code](https://github.com/eternal8080/MV-MATH)]
    *Peijie Wang, Zhongzhi Li, Dekang Ran, Fei Yin, Chenglin Liu.* CVPR 2025
    
1. [**MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models.**](https://arxiv.org/abs/2502.00698) [[project](https://acechq.github.io/MMIQ-benchmark/)] [[code](https://github.com/AceCHQ/MMIQ/tree/main/)] [[dataset](https://huggingface.co/datasets/huanqia/MM-IQ)]
    *Huanqia Cai, Yijun Yang, Winston Hu*. Preprint'25
    
1. [**MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency.**](https://arxiv.org/abs/2502.09621) [[project](https://mmecot.github.io/)] [[code](https://github.com/CaraJ7/MME-CoT)] [[dataset](https://huggingface.co/datasets/CaraJ/MME-CoT)]
    *Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li*. Preprint'25
    
1. [**ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models.**](https://arxiv.org/abs/2502.09696) [[project](https://zerobench.github.io/)] [[code](https://github.com/jonathan-roberts1/zerobench)] [[dataset](https://huggingface.co/datasets/jonathan-roberts1/zerobench)]
    *Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, David I. Atkinson, Aaditya Baranwal, Alexandru Coca, Mikah Dang, Sebastian Dziadzio, Jakob D. Kunz, Kaiqu Liang, Alexander Lo, Brian Pulfer, Steven Walton, Charig Yang, Kai Han, Samuel Albanie*. Preprint'25
    
1. [**MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning.**](https://arxiv.org/abs/2503.07365) [[code](https://github.com/ModalMinds/MM-EUREKA)] [[model](https://huggingface.co/FanqingM/MM-Eureka-8B)] [[dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)]
    *Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao.* Preprint'25
    
    

## Surveyüìñ

1. [**A Survey on Multimodal Benchmarks: In the Era of Large AI Models.**](https://arxiv.org/pdf/2409.18142) 
   *Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, Long Chen*. Preprint'24
1. [**Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey.**](https://arxiv.org/abs/2503.12605)  [[code](http://github.com/yaotingwangofficial/Awesome-MCoT)] 
   *Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, William Wang, Ziwei Liu, Jiebo Luo, Hao Fei*. Preprint'25
2. [**Mind with Eyes: from Language Reasoning to Multimodal Reasoning.**](https://arxiv.org/abs/2503.18071) 
   *Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, Jitao Sang.* Preprint'25

## Contributing‚≠ê

This is an active repository and your contributions are always welcome!
