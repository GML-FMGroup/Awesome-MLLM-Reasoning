# Awesome-MLLM-Reasoning

We have witnessed the tremendous potential of pure reinforcement learning (RL) in enhancing LLM reasoning capabilities, and a growing body of research is now extending this potential to the multimodal domain. This repository will continuously update the latest papers, covering how reinforcement learning techniques can optimize reasoning performance in multimodal tasks (e.g., visual question answering, cross-modal reasoning, and more).

**🚀 Here, you’ll be at the forefront of "RL + MLLM Reasoning" research!**
Whether you're a researcher, engineer, or student, this repository will help you quickly master **how RL empowers complex reasoning and decision-making in MLLMs**.

Watch & Star 🌟—let’s explore the future of multimodal intelligence together!

## Papers 📄

1. **[Compile Scene Graphs with Reinforcement Learning.](https://www.arxiv.org/pdf/2504.13617)** [[code](https://github.com/gpt4vision/R1-SGG)] 
    *Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen.*Preprint'25

1. **[NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation.](https://arxiv.org/abs/2504.13055)** [[code](https://github.com/John-AI-Lab/NoisyRollout)] 
    *Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, Michael Qizhe Shieh.*Preprint'25

1. **[Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning.](https://arxiv.org/abs/2504.12680)** [[project](https://embodiedcity.github.io/Embodied-R/)] [[code](https://github.com/EmbodiedCity/Embodied-R.code)] 
    *Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, Wenwu Zhu.*Preprint'25

1. **[SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement.](https://arxiv.org/abs/2504.07934)** [[code](https://github.com/si0wang/ThinkLite-VL)] 
    *Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang. Preprint'25*

1. **[Perception-R1: Pioneering Perception Policy with Reinforcement Learning.](https://arxiv.org/abs/2504.07954)** [[code](https://github.com/linkangheng/PR1)] 
    *En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, Wenbing Tao*. Preprint'25

1. **[VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning.](https://arxiv.org/abs/2504.08837)** [[project](https://tiger-ai-lab.github.io/VL-Rethinker/)] [[code](https://github.com/TIGER-AI-Lab/VL-Rethinker/)]  [[model](https://huggingface.co/TIGER-Lab/VL-Rethinker-7B)]
    *Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen*. Preprint'25

1. **[Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning.](https://arxiv.org/abs/2503.16188v2)** [[code](https://github.com/minglllli/CLS-RL)] 
    *Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Kaipeng Zhang.*Preprint'25

1. **[VisRL: Intention-Driven Visual Perception via Reinforced Reasoning.](https://arxiv.org/abs/2503.07523)** [[code](https://github.com/zhangquanchen/VisRL)] 
    *Zhangquan Chen, Xufang Luo, Dongsheng Li.* Preprint'25

1. **[Improved Visual-Spatial Reasoning via R1-Zero-Like Training.](https://arxiv.org/pdf/2504.00883)** [[code](https://github.com/zhijie-group/R1-Zero-VSI)] 
    *Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, Zhijie Deng*. Preprint'25

1. **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning.](https://arxiv.org/abs/2503.22679)**  [[code](https://github.com/lwq20020127/Q-Insight)] 
    *Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang*. Preprint'25
    
1. **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning.](https://arxiv.org/pdf/2503.21620)** [[code](https://github.com/lll6gg/UI-R1)] 
    *Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, Hongsheng Li*. Preprint'25
    
1. **[Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning.](https://arxiv.org/abs/2504.00907)**
    *Ram Ramrakhya, Matthew Chang, Xavier Puig, Ruta Desai, Zsolt Kira, Roozbeh Mottaghi.* Preprint'25
    
1. **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1.](https://arxiv.org/pdf/2503.24376)**
    *Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu.* Preprint'25
    
1. **[Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models.](https://arxiv.org/pdf/2503.16724)**
    *Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay.* Preprint'25
    
1. **[GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing.](https://arxiv.org/abs/2503.10639)** 
    *Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li.* Preprint'25
    
1. **[Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme.](https://www.arxiv.org/abs/2504.02587)** [[code](https://github.com/GAIR-NLP/MAYE)]
    *Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, Pengfei Liu.* Preprint'25
    
1. **[LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs.](https://arxiv.org/abs/2501.06186)** [[project](https://mbzuai-oryx.github.io/LlamaV-o1/)] [[code](https://github.com/mbzuai-oryx/LlamaV-o1)] [[model](https://huggingface.co/omkarthawakar/LlamaV-o1)]
    *Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan.* Preprint'25
    
1. [**LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL.**](https://arxiv.org/pdf/2503.07536) [[code](https://github.com/TideDra/lmm-r1)]
     *Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang.* Preprint'25

1. [**R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model.**](https://arxiv.org/abs/2503.05132)[[code](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)] [[model](https://huggingface.co/turningpoint-ai/VisualThinker-R1-Zero)]
     *Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, Xu Yang.* Preprint'25

1. [**R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning.**](https://arxiv.org/abs/2503.05379) [[code](https://github.com/HumanMLLM/R1-Omni)] [[model](https://huggingface.co/StarJiaxing/R1-Omni-0.5B)]
     *Jiaxing Zhao, Xihan Wei, Liefeng Bo*. Preprint'25

1. [**Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement.**](https://arxiv.org/abs/2503.06520) [[code](https://github.com/dvlab-research/Seg-Zero)] [[model](https://huggingface.co/Ricky06662/Seg-Zero-7B)]
     *Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia*. Preprint'25

1. [**Visual-RFT: Visual Reinforcement Fine-Tuning.**](https://arxiv.org/abs/2503.01785) [[code](https://github.com/Liuziyu77/Visual-RFT)]
     *Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang*. Preprint'25

1. [**MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning.**](https://arxiv.org/abs/2502.19634)
     *Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, Daniel Rueckert*. Preprint'25

1. [**Virgo: A Preliminary Exploration on Reproducing o1-like MLLM.**](https://arxiv.org/abs/2501.01904v2)[[code](https://github.com/RUCAIBox/Virgo)]
     *Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen.* Preprint'25

1. [**Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step.**](https://arxiv.org/abs/2501.13926)[[code](https://github.com/ZiyuGuo99/Image-Generation-CoT)] [[model](https://huggingface.co/ZiyuG/Image-Generation-CoT)]
     Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng. Preprint'25

1. [**R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization.**](https://arxiv.org/abs/2503.10615)[[code](https://github.com/Fancy-MLLM/R1-Onevision)] [[model](https://huggingface.co/spaces/Fancy-MLLM/R1-Onevision)]
     *Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen.* Preprint'25

1. [**Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models.**](https://arxiv.org/abs/2503.06749) [[code](https://github.com/Osilly/Vision-R1)]
      *Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin*. Preprint'25

1. **[Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models.](https://arxiv.org/abs/2411.14432)** [[code](https://github.com/dongyh20/Insight-V)] [[model](https://huggingface.co/collections/THUdyh/insight-v-673f5e1dd8ab5f2d8d332035)]

    *Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, Ziwei Liu.* Preprint'24

1. **[LLaVA-CoT: Let Vision Language Models Reason Step-by-Step.](https://arxiv.org/abs/2411.10440)** [[code](https://github.com/PKU-YuanGroup/LLaVA-CoT)] [[model](https://huggingface.co/Xkev/Llama-3.2V-11B-cot)]

    *Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, Li Yuan.* Preprint'24

1. **[Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models.](https://arxiv.org/abs/2406.09403)** [[project](https://visualsketchpad.github.io/)] [[code](https://github.com/Yushi-Hu/VisualSketchpad)]
    *Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna.* Preprint'24
    
1. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319)[[code](https://github.com/HJYao00/Mulberry)] 
    *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*. Preprint'24
    
1. [**Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization.**](https://arxiv.org/abs/2411.10442)[[project](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/)] [[code](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)] [[model](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO)]
    *Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, Jifeng Dai*. Preprint'24
    
1. [**Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search.**](https://arxiv.org/abs/2412.18319)[[code](https://github.com/HJYao00/Mulberry)] 
    *Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao*. Preprint'24



## Benchmark & Evaluation🤗



1. **[GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning.](https://arxiv.org/abs/2504.12597)** 
   *Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng.*Preprint'25
   
1. **[VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models.](https://arxiv.org/abs/2503.23064)** [[project](https://yufan-ren.com/subpage/VGRP-Bench/)] [[code](https://github.com/ryf1123/VGRP-Bench)] [[dataset](https://huggingface.co/datasets/VGRP-Bench/VGRP-Bench)]
   *Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine Süsstrunk, Filippos Kokkinos.*Preprint'25
   
1. **[VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning.](https://arxiv.org/abs/2504.05782)** [[project](https://vlm-reasoning.github.io/VCR-Bench/)]
   *Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, Feng Zhao.* Preprint'25
   
1. **[MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models.](https://arxiv.org/abs/2504.05782)** [[dataset](https://github.com/LanceZPF/MDK12)]
   
    *Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, Jianwen Sun, Haoquan Zhang, Zizhen Li, Xiaofeng Mao, Wangbo Zhao, Kai Wang, Xiaojun Chang, Wenqi Shao, Yang You, Kaipeng Zhang.* Preprint'25
    
1. **[LIVEVQA: Live Visual Knowledge Seeking.](https://arxiv.org/pdf/2504.05288)** [[dataset](https://huggingface.co/papers/2504.05288)]
    *Mingyang Fu, Yuyang Peng, Benlin Liu, Yao Wan, Dongping Chen.* CVPR 2025
    
1. **[MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts.](https://arxiv.org/abs/2502.20808)** [[project](https://eternal8080.github.io/MV-MATH.github.io/)] [[code](https://github.com/eternal8080/MV-MATH)]
    *Peijie Wang, Zhongzhi Li, Dekang Ran, Fei Yin, Chenglin Liu.* CVPR 2025
    
1. [**MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models.**](https://arxiv.org/abs/2502.00698) [[project](https://acechq.github.io/MMIQ-benchmark/)] [[code](https://github.com/AceCHQ/MMIQ/tree/main/)] [[dataset](https://huggingface.co/datasets/huanqia/MM-IQ)]
    *Huanqia Cai, Yijun Yang, Winston Hu*. Preprint'25

1. [**MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency.**](https://arxiv.org/abs/2502.09621) [[project](https://mmecot.github.io/)] [[code](https://github.com/CaraJ7/MME-CoT)] [[dataset](https://huggingface.co/datasets/CaraJ/MME-CoT)]
    *Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li*. Preprint'25

1. [**ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models.**](https://arxiv.org/abs/2502.09696) [[project](https://zerobench.github.io/)] [[code](https://github.com/jonathan-roberts1/zerobench)] [[dataset](https://huggingface.co/datasets/jonathan-roberts1/zerobench)]
    *Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, David I. Atkinson, Aaditya Baranwal, Alexandru Coca, Mikah Dang, Sebastian Dziadzio, Jakob D. Kunz, Kaiqu Liang, Alexander Lo, Brian Pulfer, Steven Walton, Charig Yang, Kai Han, Samuel Albanie*. Preprint'25

1. [**MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning.**](https://arxiv.org/abs/2503.07365) [[code](https://github.com/ModalMinds/MM-EUREKA)] [[model](https://huggingface.co/FanqingM/MM-Eureka-8B)] [[dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)]
    *Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao.* Preprint'25

1. [**MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts.**](https://arxiv.org/abs/2310.02255) [[project](https://mathvista.github.io/)] 
    *Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao.* ICLR 2024

1. [**Template-Driven LLM-Paraphrased Framework for Tabular Math Word Problem Generation.**](https://arxiv.org/abs/2412.15594v1) [[project](https://acechq.github.io/MMIQ-benchmark/)] [[code](https://github.com/AceCHQ/MMIQ/tree/main/)] [[dataset](https://huggingface.co/datasets/huanqia/MM-IQ)]
    *Xiaoqiang Kang, Zimu Wang, Xiaobo Jin, Wei Wang, Kaizhu Huang, Qiufeng Wang.* Preprint'24

1. [**BLINK: Multimodal Large Language Models Can See but Not Perceive.**](https://arxiv.org/abs/2404.12390) [[project](https://zeyofu.github.io/blink/)] [[code](https://github.com/zeyofu/BLINK_Benchmark)] [[dataset](https://huggingface.co/datasets/BLINK-Benchmark/BLINK)]
    *Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna*. ECCV 2024

1. **[HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks.](https://arxiv.org/abs/2410.12381)** [[code](https://github.com/HumanEval-V/HumanEval-V-Benchmark)] 
    *Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, Jacky Keung.* Preprint'24

1. [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension.**](https://arxiv.org/abs/2307.16125) [[code](https://github.com/AILab-CVC/SEED-Bench)]
    *Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan*. Preprint'23

1. [**ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning.**](https://arxiv.org/abs/2203.10244) [[code](https://github.com/vis-nlp/ChartQA)]
    *Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, Enamul Hoque.* Preprint'22

1. [**Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.**](http://arxiv.org/abs/2209.09513) [[code](https://github.com/lupantech/ScienceQA)]
     *Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan.* NeurIPS 2022

1. [***GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning*.**](https://arxiv.org/abs/2105.14517)  [[code](https://github.com/chen-judge/GeoQA)]
     *Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, Liang Lin.* Preprint'21

1. [**FigureQA: An Annotated Figure Dataset for Visual Reasoning.**](https://arxiv.org/abs/1710.07300)
    *Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, Yoshua Bengio.*  Preprint'17
    
    

## Survey📖

1. [**Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey.**](https://arxiv.org/abs/2503.12605)  [[code](http://github.com/yaotingwangofficial/Awesome-MCoT)] 
   *Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, William Wang, Ziwei Liu, Jiebo Luo, Hao Fei*. Preprint'25
2. [**Mind with Eyes: from Language Reasoning to Multimodal Reasoning.**](https://arxiv.org/abs/2503.18071) 
   *Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, Jitao Sang.* Preprint'25
1. [**A Survey on Multimodal Benchmarks: In the Era of Large AI Models.**](https://arxiv.org/pdf/2409.18142) 
   *Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, Long Chen*. Preprint'24

## Contributing⭐

This is an active repository and your contributions are always welcome!
